{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "train_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv29DzlL5vN3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np, random\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import KeyedVectors\n",
        "import datetime\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Embedding, Activation, Dense, Concatenate, Reshape\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback,ModelCheckpoint, EarlyStopping\n",
        "from time import time\n",
        "from keras.models import load_model, Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DQDZxSn6M7s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "10554a8d-e915-48f9-f202-eec2908b123b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn1Q3Y_75vOD"
      },
      "source": [
        "file = '/gdrive/My Drive/NLA_project/test_data.csv' \n",
        "data = pd.read_csv(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTF55CHb5vOI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "caa4a7ec-ca29-49b6-c4af-8184f44ab036"
      },
      "source": [
        "data = data[['Cit_Text', 'Ref_text', 'label']]\n",
        "print(len(data[data['label']==1]),len(data[data['label']==0]))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15492 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cit_Text</th>\n",
              "      <th>Ref_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>levin s classification extended nlp researcher...</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>levin s classification extended nlp researcher...</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dang et al</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>modify adding new class remove overlap class o...</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>levin s classification extended nlp researcher...</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Cit_Text  ... label\n",
              "0  levin s classification extended nlp researcher...  ...     1\n",
              "1  levin s classification extended nlp researcher...  ...     1\n",
              "2                                         dang et al  ...     1\n",
              "3  modify adding new class remove overlap class o...  ...     1\n",
              "4  levin s classification extended nlp researcher...  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZEdas3M5vOQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "27b24afd-04c3-4959-f720-f707718a9099"
      },
      "source": [
        "#### CONSIDER REMOVING SENTENCE LENGTHS ABOVE 100\n",
        "lower_lim = 3\n",
        "upper_lim = 40\n",
        "\n",
        "\n",
        "a, b, c = (data.values)[0]\n",
        "print(str(a), str(b), str(c))\n",
        "\n",
        "lis = list()\n",
        "for a,b,c in data.values:\n",
        "    a = str(a).split(' ')\n",
        "    b = str(b).split(' ')\n",
        "    \n",
        "    if len(a) < lower_lim or len(b) < lower_lim:\n",
        "        continue\n",
        "    if len(a) > upper_lim:\n",
        "        a = a[:upper_lim]\n",
        "    if len(b) > upper_lim:\n",
        "        b = b[:upper_lim]\n",
        "        \n",
        "    lis.append((' '.join(a),' '.join(b),c))\n",
        "\n",
        "print(len(lis),len(data))\n",
        "data = pd.DataFrame(lis , columns = ['Cit_Text', 'Ref_text', 'label'])\n",
        "print(len(data[data['label']==1]),len(data[data['label']==0]))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "levin s classification extended nlp researcher dorr jones dang et al augmented existing database levin semantic class set intersective class created grouping to gether subset existing class over lapping member 1\n",
            "15386 15492\n",
            "15386 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cit_Text</th>\n",
              "      <th>Ref_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>levin s classification extended nlp researcher...</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>levin s classification extended nlp researcher...</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dang et al</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>modify adding new class remove overlap class o...</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>levin s classification extended nlp researcher...</td>\n",
              "      <td>augmented existing database levin semantic cla...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Cit_Text  ... label\n",
              "0  levin s classification extended nlp researcher...  ...     1\n",
              "1  levin s classification extended nlp researcher...  ...     1\n",
              "2                                         dang et al  ...     1\n",
              "3  modify adding new class remove overlap class o...  ...     1\n",
              "4  levin s classification extended nlp researcher...  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDvxpO6V5vOn"
      },
      "source": [
        "### CROSS VALIDATION\n",
        "\n",
        "# X1 = data['Cit_Text'].values\n",
        "# X2 = data['Ref_text'].values\n",
        "# y=data['label'].values\n",
        "\n",
        "# skf = StratifiedKFold(n_splits=2)\n",
        "# skf.get_n_splits(X, y)\n",
        "\n",
        "# print(skf)\n",
        "\n",
        "# for train_index, test_index in skf.split(X, y):\n",
        "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "#     X_train, X_test = X[train_index], X[test_index]\n",
        "#     y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2LQcQTE5vOs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "929f74c0-5a04-42e1-d1a2-e6101516002f"
      },
      "source": [
        "max_len = 41\n",
        "def load_data_and_labels():\n",
        "\n",
        "    cit_text = data['Cit_Text'].values\n",
        "    ref_text = data['Ref_text'].values\n",
        "\n",
        "    cit_text = [str(temp).split(' ') for temp in cit_text]\n",
        "    ref_text = [str(temp).split(' ') for temp in ref_text]\n",
        "    \n",
        "    x_text = [zipped for zipped in zip(cit_text, ref_text)]\n",
        "    return x_text, data['label'].values\n",
        "\n",
        "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
        "    \n",
        "    global max_len\n",
        "    max_len = max(max(len(x[0]), len(x[1])) for x in sentences)\n",
        "    min_len =  min(min(len(x[0]), len(x[1])) for x in sentences)\n",
        "    max_len += 1\n",
        "\n",
        "    padded_sentences = list()\n",
        "    for a,b in sentences:\n",
        "        a += [padding_word] * (max_len - len(a))\n",
        "        b += [padding_word] * (max_len - len(b))\n",
        "        padded_sentences.append((a, b))\n",
        "    return padded_sentences\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    \n",
        "    cit = list()\n",
        "    ref = list()\n",
        "    for a, b in sentences:\n",
        "        cit.append(a)\n",
        "        ref.append(b)\n",
        "    word_counts = Counter(itertools.chain(*cit))\n",
        "    dict.update(word_counts, Counter(itertools.chain(*ref)))\n",
        "\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "def build_input_data(sentences, labels, vocabulary):\n",
        "    x1 = []\n",
        "    for sentence in sentences:\n",
        "        temp = []\n",
        "        for word in sentence[0]:\n",
        "            if str(word) in vocabulary:\n",
        "                temp.append(vocabulary[str(word)])\n",
        "            else:\n",
        "                temp.append(random.randint(0, max_len))\n",
        "        x1.append(temp)\n",
        "    x2 = []\n",
        "    for sentence in sentences:\n",
        "        temp = []\n",
        "        for word in sentence[1]:\n",
        "            if str(word) in vocabulary:\n",
        "                temp.append(vocabulary[str(word)])\n",
        "            else:\n",
        "                temp.append(random.randint(0, max_len))\n",
        "        x2.append(temp)\n",
        "    # x1 = np.array([[vocabulary[str(word)] for word in sentence[0]] for sentence in sentences])\n",
        "    # x2 = np.array([[vocabulary[str(word)] for word in sentence[1]] for sentence in sentences])\n",
        "    y = np.array(labels)\n",
        "    return [np.array(x1), np.array(x2), y]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "\n",
        "    sentences, labels = load_data_and_labels()\n",
        "    \n",
        "    sentences = pad_sentences(sentences)\n",
        "    print(len(sentences), len(sentences[0][0]))\n",
        "    # vocabulary, vocabulary_inv = build_vocab(sentences)\n",
        "    X1, X2, y = build_input_data(sentences, labels, vocabulary)\n",
        "    print(X1[:2], X2[:2], labels[:2])\n",
        "    # return [X1, X2, y, vocabulary, vocabulary_inv]\n",
        "    return X1,X2,y\n",
        "\n",
        "X1, X2, y = load_data()\n",
        "# X1, X2, y, vocabulary, vocabulary_inv = load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15386 41\n",
            "[[ 227  420  487 1623 1673 3170 3443 3297  222   30   29    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [ 227  420  487 1623 1673 3170 3443 3297  222   30   29    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]] [[ 302  557  754  227   25   37   13  669   37  590 1003   37 1218  472\n",
            "   557   37  872 1219 1081    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [ 302  557  754  227   25   37   13  669   37  590 1003   26 1218  472\n",
            "   557   37  872 1219 1081    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0]] [1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHzoEtCT5vOw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e3b20d9-3651-483b-fbe7-1ef5a7001ccc"
      },
      "source": [
        "# EMBEDDING_FILE = '..../GoogleNews-vectors-negative300.bin.gz'\n",
        "# word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "\n",
        "embedding_dim = 300\n",
        "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)\n",
        "embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "for word, index in vocabulary.items():\n",
        "#     if word in word2vec.vocab:\n",
        "#         embeddings[index] = word2vec.word_vec(word)\n",
        "#     else:\n",
        "    embeddings[index] = np.random.uniform(-0.25,0.25,embedding_dim)\n",
        "# del word2vec\n",
        "print(embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5974, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE0iQ3bC5vO1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3adbdd4-4345-4dcd-d2d9-937c79e200e2"
      },
      "source": [
        "X = list()\n",
        "for a, b in zip(X1, X2):\n",
        "    X.append((a, b))\n",
        "X_train,X_test, Y_train, Y_test =  train_test_split(X, y,test_size = 0.20,random_state= 4)\n",
        "\n",
        "X_train1 = list()\n",
        "X_train2 = list()\n",
        "\n",
        "X_test1 = list()\n",
        "X_test2 = list()\n",
        "\n",
        "for a, b in X_train:\n",
        "    X_train1.append(a)\n",
        "    X_train2.append(b)\n",
        "    \n",
        "for a, b in X_test:\n",
        "    X_test1.append(a)\n",
        "    X_test2.append(b)\n",
        "    \n",
        "X_train1 = np.array(X_train1)\n",
        "X_train2 = np.array(X_train2)\n",
        "X_test1 = np.array(X_test1)\n",
        "X_test2 = np.array(X_test2)\n",
        "Y_train = np.array(Y_train)\n",
        "Y_test = np.array(Y_test)\n",
        "print(len(X_train1), len(X_train2), len(Y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46731 46731 46731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX_dEZTC5vO6"
      },
      "source": [
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    # precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    # f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return recall\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    # recall = true_positives / (possible_positives + K.epsilon())\n",
        "    # f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return precision\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm8Mjbmn5vO_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "ad9a78cb-d522-4cb0-de7d-67c066a75516"
      },
      "source": [
        "# n_epoch = 5\n",
        "\n",
        "# left_input = Input(shape=(max_len,))\n",
        "# right_input = Input(shape=(max_len,))\n",
        "\n",
        "# embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_len, trainable=True)\n",
        "\n",
        "# encoded_left = embedding_layer(left_input)\n",
        "# encoded_right = embedding_layer(right_input)\n",
        "\n",
        "# # first_model = Sequential()\n",
        "# # first_model.add(Bidirectional (LSTM (150, return_sequences=True, dropout=0.50),merge_mode='concat'))\n",
        "# # # first_model.add(LSTM(output_dim, input_shape=(m, input_dim)))\n",
        "\n",
        "# # second_model = Sequential()\n",
        "# # # second_model.add(LSTM(output_dim, input_shape=(n-m, input_dim)))\n",
        "# # second_model.add(Bidirectional (LSTM (150, return_sequences=True, dropout=0.50),merge_mode='concat'))\n",
        "\n",
        "\n",
        "# # model = Sequential()\n",
        "# # model.add(Concatenate([first_model, second_model]))\n",
        "# # model.add(Dense(100))\n",
        "# # model.add(Dense(32))\n",
        "# # model.add(Dense(1))\n",
        "# # model.add(Activation('sigmoid'))\n",
        "# # model.compile('adam', 'binary_crossentropy', metrics=['accuracy', get_f1])\n",
        "\n",
        "# mode_path = '/gdrive/My Drive/NLA_project/models/two_lstms.h5'\n",
        "# training_start_time = time()\n",
        "\n",
        "# # model.fit([X_train1, X_train2], Y_train, epochs=n_epoch, validation_split=0.2, shuffle=True)\n",
        "\n",
        "# first_out = Bidirectional (LSTM (150, return_sequences=False, dropout=0.50),merge_mode='concat')(encoded_left)\n",
        "# # first_out = Dense(1)(first_out)\n",
        "# second_out = Bidirectional (LSTM (150, return_sequences=False, dropout=0.50),merge_mode='concat')(encoded_right)\n",
        "# # second_out = Dense(1)(second_out)\n",
        "\n",
        "# merged = Concatenate(axis=1)([first_out, second_out])\n",
        "# # merged = Reshape([1,-1])(merged)\n",
        "# output_layer1 = Dense(128)(merged)\n",
        "# output_layer2 = Dense(32)(output_layer1)\n",
        "# output_layer3 = Dense(1, activation='sigmoid')(output_layer2)\n",
        "# # output_layer1 = Reshape([None, 1])(output_layer1)\n",
        "# model = Model(inputs=[left_input, right_input], outputs=output_layer3)\n",
        "# model.compile('adam', 'binary_crossentropy', metrics=['accuracy', get_f1, recall, precision])\n",
        "# model.summary()\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
        "# # model.fit([X_train1, X_train2], Y_train, epochs=n_epoch, validation_split=0.3, shuffle=True,callbacks=[early_stopping, ModelCheckpoint(filepath=mode_path, save_best_only=True)])\n",
        "# print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\n",
        "\n",
        "saved_model = load_model(mode_path, custom_objects={\"get_f1\":get_f1, \"precision\":precision, \"recall\":recall})\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['accuracy', get_f1, recall, precision])\n",
        "result = model.predict([X1, X2])\n",
        "ans=0\n",
        "for res in result:\n",
        "  if res>0.5:\n",
        "    ans+=1\n",
        "print(ans, result)\n",
        "model.evaluate([X1, X2], y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14168 [[9.999887e-01]\n",
            " [9.999888e-01]\n",
            " [9.982508e-01]\n",
            " ...\n",
            " [9.007040e-05]\n",
            " [9.786330e-01]\n",
            " [2.692158e-02]]\n",
            "15386/15386 [==============================] - 19s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5876500380610332,\n",
              " 0.9208371043205261,\n",
              " 0.9575027823448181,\n",
              " 0.9208080172538757,\n",
              " 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}