{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv29DzlL5vN3"
      },
      "source": [
        "# !pip3 install --quiet tensorflow==1.15\n",
        "# !pip3 install --quiet \"tensorflow_hub>=0.6.0\"\n",
        "# !pip3 install --quiet tensorflow_text==1.15\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from time import time\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from keras.models import load_model, Model, Sequential\n",
        "from keras.layers import Bidirectional, Input, Embedding, Activation, Dense, Concatenate, Reshape, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback,ModelCheckpoint, EarlyStopping\n",
        "from keras.layers import Flatten, Lambda\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.pooling import MaxPooling1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DQDZxSn6M7s",
        "outputId": "18d0a855-53c2-45f1-ecb9-824e962e9995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn1Q3Y_75vOD"
      },
      "source": [
        "file = '/gdrive/My Drive/NLA_project/new/data_unstemmed.csv' \n",
        "vocab_path = '/gdrive/My Drive/NLA_project/new/vocabulary_full'\n",
        "model_path = '/gdrive/My Drive/NLA_project/new/Siamese_CNN_univ_man.h5'\n",
        "# data = pd.read_csv(file, names=[\"Ref_paper\", \"Cit_paper\", \"Cit_Text\", \"Ref_text\", \"label\"])\n",
        "\n",
        "data = pd.read_csv(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTF55CHb5vOI",
        "outputId": "fd3721a2-b2bd-4357-9a04-7397c9259982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "source": [
        "data = data[['Cit_Text', 'Ref_text', 'label']]\n",
        "print(len(data[data['label']==1]),len(data[data['label']==0]))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33753 29989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cit_Text</th>\n",
              "      <th>Ref_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>finally use pos tagger trained tweets gimpel e...</td>\n",
              "      <td>contributions follows developed features twitt...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>finally use pos tagger trained tweets gimpel e...</td>\n",
              "      <td>next obtained random sample mostly american en...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>finally use pos tagger trained tweets gimpel e...</td>\n",
              "      <td>figure 1 show examples certain features help s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>finally use pos tagger trained tweets gimpel e...</td>\n",
              "      <td>one fundamental parts linguistic pipeline part...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>however pos taggers twitter available limited ...</td>\n",
              "      <td>pos taggers trained treebanks newswire domain ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Cit_Text  ... label\n",
              "0  finally use pos tagger trained tweets gimpel e...  ...     1\n",
              "1  finally use pos tagger trained tweets gimpel e...  ...     1\n",
              "2  finally use pos tagger trained tweets gimpel e...  ...     0\n",
              "3  finally use pos tagger trained tweets gimpel e...  ...     0\n",
              "4  however pos taggers twitter available limited ...  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZEdas3M5vOQ",
        "outputId": "af789dee-ddfe-4a18-f2ad-73ec6fe91788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "#### CONSIDER REMOVING SENTENCE LENGTHS ABOVE 100\n",
        "lower_lim = 3\n",
        "upper_lim = 40\n",
        "\n",
        "\n",
        "a, b, c = (data.values)[0]\n",
        "print(str(a), str(b), str(c))\n",
        "\n",
        "lis = list()\n",
        "for a,b,c in data.values:\n",
        "    a = str(a).split(' ')\n",
        "    b = str(b).split(' ')\n",
        "    \n",
        "    if len(a) < lower_lim or len(b) < lower_lim:\n",
        "        continue\n",
        "    if len(a) > upper_lim:\n",
        "        a = a[:upper_lim]\n",
        "    if len(b) > upper_lim:\n",
        "        b = b[:upper_lim]\n",
        "        \n",
        "    lis.append((' '.join(a),' '.join(b),c))\n",
        "\n",
        "print(len(lis),len(data))\n",
        "data = pd.DataFrame(lis , columns = ['Cit_Text', 'Ref_text', 'label'])\n",
        "print(len(data[data['label']==1]),len(data[data['label']==0]))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finally use pos tagger trained tweets gimpel et al 2011 perform pos tagging tweets data apart previously recognised named entities words tagged nouns verbs adjectives kept contributions follows developed features twitter pos tagging conducted experiments evaluate provide annotated corpus trained pos tagger research community 1\n",
            "59654 63742\n",
            "29917 29737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cit_Text</th>\n",
              "      <th>Ref_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>finally use pos tagger trained tweets gimpel e...</td>\n",
              "      <td>contributions follows developed features twitt...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>finally use pos tagger trained tweets gimpel e...</td>\n",
              "      <td>next obtained random sample mostly american en...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>finally use pos tagger trained tweets gimpel e...</td>\n",
              "      <td>figure 1 show examples certain features help s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>finally use pos tagger trained tweets gimpel e...</td>\n",
              "      <td>one fundamental parts linguistic pipeline part...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>however pos taggers twitter available limited ...</td>\n",
              "      <td>pos taggers trained treebanks newswire domain ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Cit_Text  ... label\n",
              "0  finally use pos tagger trained tweets gimpel e...  ...     1\n",
              "1  finally use pos tagger trained tweets gimpel e...  ...     1\n",
              "2  finally use pos tagger trained tweets gimpel e...  ...     0\n",
              "3  finally use pos tagger trained tweets gimpel e...  ...     0\n",
              "4  however pos taggers twitter available limited ...  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDvxpO6V5vOn"
      },
      "source": [
        "### CROSS VALIDATION\n",
        "\n",
        "# X1 = data['Cit_Text'].values\n",
        "# X2 = data['Ref_text'].values\n",
        "# y=data['label'].values\n",
        "\n",
        "# skf = StratifiedKFold(n_splits=2)\n",
        "# skf.get_n_splits(X, y)\n",
        "\n",
        "# print(skf)\n",
        "\n",
        "# for train_index, test_index in skf.split(X, y):\n",
        "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "#     X_train, X_test = X[train_index], X[test_index]\n",
        "#     y_train, y_test = y[train_index], y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2LQcQTE5vOs",
        "outputId": "88b950b0-0c57-4b79-e142-84fe2bab8612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "max_len = 41\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib qt\n",
        "def load_data_and_labels():\n",
        "\n",
        "    cit_text = data['Cit_Text'].values\n",
        "    ref_text = data['Ref_text'].values\n",
        "\n",
        "    cit_text = [str(temp).split(' ') for temp in cit_text]\n",
        "    ref_text = [str(temp).split(' ') for temp in ref_text]\n",
        "    \n",
        "    x_text = [zipped for zipped in zip(cit_text, ref_text)]\n",
        "    return x_text, data['label'].values\n",
        "\n",
        "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
        "    \n",
        "    global max_len\n",
        "    max_len = max(max(len(x[0]), len(x[1])) for x in sentences)\n",
        "    min_len =  min(min(len(x[0]), len(x[1])) for x in sentences)\n",
        "    max_len += 1\n",
        "\n",
        "    padded_sentences = list()\n",
        "    for a,b in sentences:\n",
        "        a += [padding_word] * (max_len - len(a))\n",
        "        b += [padding_word] * (max_len - len(b))\n",
        "        padded_sentences.append((a, b))\n",
        "    return padded_sentences\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    \n",
        "    cit = list()\n",
        "    ref = list()\n",
        "    for a, b in sentences:\n",
        "        cit.append(a)\n",
        "        ref.append(b)\n",
        "    word_counts = Counter(itertools.chain(*cit))\n",
        "    dict.update(word_counts, Counter(itertools.chain(*ref)))\n",
        "    X_plot = list()\n",
        "    Y_plot = list()\n",
        "    # for a,b in word_counts.most_common(1000):\n",
        "    #   if b>=100000:\n",
        "    #     continue\n",
        "    #   X_plot.append(a)\n",
        "    #   Y_plot.append(word_counts[a])\n",
        "      # print(\"Yes\")\n",
        "    # plt.plot(X_plot, Y_plot)\n",
        "    # plt.show()\n",
        "    till = 0 \n",
        "    for a,b in word_counts.most_common():\n",
        "      if b<=5:\n",
        "        break\n",
        "      till += 1\n",
        "      # print(a,b)\n",
        "    print(len(word_counts.most_common(till)))\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common(till)]\n",
        "    vocabulary_inv.append('<UNK>')\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "def build_input_data(sentences, labels, vocabulary):\n",
        "    \n",
        "    x1 = []\n",
        "    for sentence in sentences:\n",
        "        temp = []\n",
        "        for word in sentence[0]:\n",
        "            if str(word) in vocabulary:\n",
        "                temp.append(vocabulary[str(word)])\n",
        "            else:\n",
        "                temp.append(vocabulary['<UNK>'])\n",
        "        x1.append(temp)\n",
        "    x2 = []\n",
        "    for sentence in sentences:\n",
        "        temp = []\n",
        "        for word in sentence[1]:\n",
        "            if str(word) in vocabulary:\n",
        "                temp.append(vocabulary[str(word)])\n",
        "            else:\n",
        "                temp.append(vocabulary['<UNK>'])\n",
        "        x2.append(temp)\n",
        "    y = np.array(labels)\n",
        "    return [np.array(x1), np.array(x2), y]\n",
        "\n",
        "def save_vocab(vocabulary):\n",
        "    dbfile = open(vocab_path, 'wb')\n",
        "    pickle.dump(vocabulary, dbfile)                      \n",
        "    dbfile.close()\n",
        "\n",
        "\n",
        "def load_data():\n",
        "\n",
        "    sentences, labels = load_data_and_labels()\n",
        "    \n",
        "    sentences = pad_sentences(sentences)\n",
        "    print(len(sentences), len(sentences[0][0]))\n",
        "    \n",
        "    vocabulary, vocabulary_inv = build_vocab(sentences)\n",
        "    save_vocab(vocabulary)\n",
        "    \n",
        "    X1, X2, y = build_input_data(sentences, labels, vocabulary)\n",
        "    print(X1[:2], X2[:2], labels[:2])\n",
        "    return [X1, X2, y, vocabulary, vocabulary_inv]\n",
        "\n",
        "X1, X2, y, vocabulary, vocabulary_inv = load_data()\n",
        "# load_data()\n",
        "print(len(list(vocabulary.keys())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59654 41\n",
            "10310\n",
            "[[  369    14   151   201   156  2694  5834     2     1   765   420   151\n",
            "    131  2694     9  3104   494 10310   344   300     7   460   292   233\n",
            "    669  2742     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0]\n",
            " [  369    14   151   201   156  2694  5834     2     1   765   420   151\n",
            "    131  2694     9  3104   494 10310   344   300     7   460   292   233\n",
            "    669  2742     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0]] [[ 2609   363   333    23  1644   151   131  1624    86   373   346   252\n",
            "     18   156   151   201   223  1471     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0]\n",
            " [  340   244   427   449  1437  2336 10310  2694  6033  2569   365   199\n",
            "   4089     6  1644  5412  5588     2     1  4328    17  9721     6  9078\n",
            "   1750   151   201  2123     2     1    43   105  1069   805   228     0\n",
            "      0     0     0     0     0]] [1 1]\n",
            "10311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4OoZ6hpHmMf",
        "outputId": "bf3a8c8d-8a7f-4e97-cd73-b49236642013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
        "\n",
        "# Compute a representation for each message, showing various lengths supported.\n",
        "word = \"Elephant\"\n",
        "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
        "paragraph = (\n",
        "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
        "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
        "    \"the more 'diluted' the embedding will be.\")\n",
        "messages = [word, sentence, paragraph]\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "session = tf.Session()\n",
        "session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "message_embeddings = session.run(embed(messages))\n",
        "\n",
        "for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
        "  print(\"Message: {}\".format(messages[i]))\n",
        "  print(\"Embedding size: {}\".format(len(message_embedding)))\n",
        "  message_embedding_snippet = \", \".join(\n",
        "      (str(x) for x in message_embedding[:3]))\n",
        "  print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message: Elephant\n",
            "Embedding size: 512\n",
            "Embedding: [0.044984716922044754, -0.05743396654725075, 0.002211471553891897, ...]\n",
            "\n",
            "Message: I am a sentence for which I would like to get its embedding.\n",
            "Embedding size: 512\n",
            "Embedding: [0.05568017438054085, -0.009607899002730846, 0.006246284581720829, ...]\n",
            "\n",
            "Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.\n",
            "Embedding size: 512\n",
            "Embedding: [0.038749415427446365, 0.0765201598405838, -0.0007946128025650978, ...]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHzoEtCT5vOw",
        "outputId": "236331ec-c0d5-49df-9c45-4db844a6e99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "# # EMBEDDING_FILE = '..../GoogleNews-vectors-negative300.bin.gz'\n",
        "# # word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "# session = tf.Session()\n",
        "# session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "# embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
        "# print(np.array(vocabulary.items()).shape)\n",
        "embedding_dim = 512\n",
        "embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)\n",
        "embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "# embeds = session.run(embed(list(vocabulary.keys())))\n",
        "# print(embeds.shape)\n",
        "# for i, word in enumerate(list(vocabulary.keys())):\n",
        "#   embeddings[vocabulary[word]] = embeds[i]\n",
        "\n",
        "for word, index in vocabulary.items():\n",
        "#     # if word in word2vec.vocab:\n",
        "#     # embeddings[index] = np.array(embeds[index])\n",
        "#     # else:\n",
        "    embeddings[index] = np.random.uniform(-0.25,0.25,embedding_dim)\n",
        "# del word2vec\n",
        "print(embeddings.shape)\n",
        "# session.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10312, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuWOvbl_oph7"
      },
      "source": [
        "import pickle\n",
        "embed_f = '/gdrive/My Drive/NLA_project/embeddings_univ'\n",
        "e_f = open(embed_f, 'wb')\n",
        "pickle.dump(embeddings, e_f)\n",
        "e_f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE0iQ3bC5vO1",
        "outputId": "e245f03f-d825-4728-b8d0-44fac3d204a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = list()\n",
        "for a, b in zip(X1, X2):\n",
        "    X.append((a, b))\n",
        "X_train,X_test, Y_train, Y_test =  train_test_split(X, y,test_size = 0.20,random_state= 4,shuffle=False)\n",
        "\n",
        "training = []\n",
        "for a,b in zip(X_train, Y_train):\n",
        "    training.append((a,b))\n",
        "np.random.shuffle(training)\n",
        "\n",
        "X_train1 = list()\n",
        "X_train2 = list()\n",
        "Y_train = list()\n",
        "\n",
        "X_test1 = list()\n",
        "X_test2 = list()\n",
        "\n",
        "for a, b in training:\n",
        "    X_train1.append(a[0])\n",
        "    X_train2.append(a[1])\n",
        "    Y_train.append(b)\n",
        "    \n",
        "for a, b in X_test:\n",
        "    X_test1.append(a)\n",
        "    X_test2.append(b)\n",
        "    \n",
        "X_train1 = np.array(X_train1)\n",
        "X_train2 = np.array(X_train2)\n",
        "X_test1 = np.array(X_test1)\n",
        "X_test2 = np.array(X_test2)\n",
        "Y_train = np.array(Y_train)\n",
        "Y_test = np.array(Y_test)\n",
        "print(len(X_train1), len(X_train2), len(Y_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47723 47723 47723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX_dEZTC5vO6"
      },
      "source": [
        "def get_f1(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm8Mjbmn5vO_",
        "outputId": "eb3b85fd-18d7-4f3a-eacc-04991d6702fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "n_epoch = 10\n",
        "\n",
        "left_input = Input(shape=(max_len,))\n",
        "right_input = Input(shape=(max_len,))\n",
        "\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_len, trainable=True)\n",
        "\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "encoder_lstm = Bidirectional (LSTM (300, return_sequences=False, dropout=0.50),merge_mode='concat')\n",
        "first_out = encoder_lstm (encoded_left)\n",
        "second_out = encoder_lstm (encoded_right)\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "# malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([first_out, second_out])\n",
        "merged = Concatenate(axis=1)([first_out, second_out])\n",
        "\n",
        "output_layer1 = Dense(128, activation='relu')(merged)\n",
        "output_layer2 = Dense(32, activation='relu')(output_layer1)\n",
        "output_layer3 = Dense(1, activation='sigmoid')(output_layer2)\n",
        "\n",
        "# model = Model(inputs=[left_input, right_input], outputs=malstm_distance)\n",
        "model = Model(inputs=[left_input, right_input], outputs=output_layer3)\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['accuracy', get_f1, recall, precision])\n",
        "model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor=\"val_get_f1\", mode='max',patience=3, verbose=True)\n",
        "\n",
        "training_start_time = time()\n",
        "\n",
        "model.fit([X_train1, X_train2], Y_train, epochs=n_epoch, validation_data=([X_test1, X_test2],Y_test),callbacks = [early_stopping, ModelCheckpoint(filepath=model_path, save_best_only=True)])\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))\n",
        "\n",
        "# saved_model = load_model(mode_path, custom_objects={\"get_f1\":get_f1, \"precision\":precision, \"recall\":recall})\n",
        "# model.compile('adam', 'binary_crossentropy', metrics=['accuracy', get_f1, recall, precision])\n",
        "# result = model.predict([X_test1, X_test2])\n",
        "# print(result)\n",
        "# model.evaluate([X_test1, X_test2], Y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           (None, 41)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           (None, 41)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 41, 512)      5279744     input_13[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 600)          1951200     embedding_7[0][0]                \n",
            "                                                                 embedding_7[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 1200)         0           bidirectional_6[0][0]            \n",
            "                                                                 bidirectional_6[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          153728      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 32)           4128        dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            33          dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 7,388,833\n",
            "Trainable params: 7,388,833\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 47723 samples, validate on 11931 samples\n",
            "Epoch 1/10\n",
            "47723/47723 [==============================] - 404s 8ms/step - loss: 0.6034 - accuracy: 0.6621 - get_f1: 0.6274 - recall: 0.6025 - precision: 0.6874 - val_loss: 0.6110 - val_accuracy: 0.6573 - val_get_f1: 0.6022 - val_recall: 0.5515 - val_precision: 0.7102\n",
            "Epoch 2/10\n",
            "47723/47723 [==============================] - 403s 8ms/step - loss: 0.5337 - accuracy: 0.7146 - get_f1: 0.6983 - recall: 0.6843 - precision: 0.7312 - val_loss: 0.6219 - val_accuracy: 0.6526 - val_get_f1: 0.6125 - val_recall: 0.5798 - val_precision: 0.6920\n",
            "Epoch 3/10\n",
            "47723/47723 [==============================] - 403s 8ms/step - loss: 0.4794 - accuracy: 0.7537 - get_f1: 0.7474 - recall: 0.7534 - precision: 0.7548 - val_loss: 0.6330 - val_accuracy: 0.6392 - val_get_f1: 0.6188 - val_recall: 0.6098 - val_precision: 0.6595\n",
            "Epoch 4/10\n",
            "47723/47723 [==============================] - 402s 8ms/step - loss: 0.4170 - accuracy: 0.7933 - get_f1: 0.7896 - recall: 0.8015 - precision: 0.7897 - val_loss: 0.7067 - val_accuracy: 0.6396 - val_get_f1: 0.6113 - val_recall: 0.5908 - val_precision: 0.6642\n",
            "Epoch 5/10\n",
            "47723/47723 [==============================] - 403s 8ms/step - loss: 0.3541 - accuracy: 0.8337 - get_f1: 0.8310 - recall: 0.8425 - precision: 0.8297 - val_loss: 0.7520 - val_accuracy: 0.6281 - val_get_f1: 0.6029 - val_recall: 0.5842 - val_precision: 0.6474\n",
            "Epoch 6/10\n",
            "47723/47723 [==============================] - 403s 8ms/step - loss: 0.2936 - accuracy: 0.8674 - get_f1: 0.8646 - recall: 0.8746 - precision: 0.8624 - val_loss: 0.8258 - val_accuracy: 0.6266 - val_get_f1: 0.6129 - val_recall: 0.6105 - val_precision: 0.6371\n",
            "Epoch 00006: early stopping\n",
            "Training time finished.\n",
            "10 epochs in 0:40:40.896616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32YnKo9fntij",
        "outputId": "837495ad-f30e-4295-8d09-cc69200b5113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_epoch = 10\n",
        "kernel_size = 5\n",
        "filters = 128\n",
        "pool_size = 4\n",
        "\n",
        "lstm_output_size = 128\n",
        "\n",
        "model_path = '/gdrive/My Drive/NLA_project/new/Siamese_LSTM_univ.h5'\n",
        "\n",
        "left_input = Input(shape=(max_len,))\n",
        "right_input = Input(shape=(max_len,))\n",
        "\n",
        "embedding_layer = Embedding(len(embeddings), embedding_dim, weights=[embeddings], input_length=max_len, trainable=True)\n",
        "\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "encoded_left = Dropout(0.25) (encoded_left)\n",
        "encoded_right = Dropout(0.25) (encoded_right)\n",
        "\n",
        "cnn_share = Conv1D(filters,kernel_size,padding='valid',activation='relu',strides=1)\n",
        "\n",
        "encoded_left1 = cnn_share (encoded_left)\n",
        "encoded_right1 = cnn_share (encoded_right)\n",
        "\n",
        "first_out1 = Flatten() (MaxPooling1D(pool_size=pool_size) (encoded_left1))\n",
        "second_out1 = Flatten() (MaxPooling1D(pool_size=pool_size) (encoded_right1))\n",
        "\n",
        "merged1 = Concatenate(axis=1)([first_out1, second_out1])\n",
        "\n",
        "\n",
        "cnn_share2 = Conv1D(filters,3,padding='valid',activation='relu',strides=1)\n",
        "\n",
        "encoded_left2 = cnn_share2 (encoded_left)\n",
        "encoded_right2 = cnn_share2 (encoded_right)\n",
        "\n",
        "first_out2 = Flatten() (MaxPooling1D(pool_size=pool_size) (encoded_left2))\n",
        "second_out2 = Flatten() (MaxPooling1D(pool_size=pool_size) (encoded_right2))\n",
        "\n",
        "merged2 = Concatenate(axis=1)([first_out2, second_out2])\n",
        "\n",
        "cnn_share3 = Conv1D(filters,4,padding='valid',activation='relu',strides=1)\n",
        "\n",
        "encoded_left3 = cnn_share3 (encoded_left)\n",
        "encoded_right3 = cnn_share3 (encoded_right)\n",
        "\n",
        "first_out3 = Flatten() (MaxPooling1D(pool_size=pool_size) (encoded_left3))\n",
        "second_out3 = Flatten() (MaxPooling1D(pool_size=pool_size) (encoded_right3))\n",
        "\n",
        "merged3 = Concatenate(axis=1)([first_out3, second_out3])\n",
        "\n",
        "cnn_share4 = Conv1D(filters,6,padding='valid',activation='relu',strides=1)\n",
        "\n",
        "encoded_left4 = cnn_share4 (encoded_left)\n",
        "encoded_right4 = cnn_share4 (encoded_right)\n",
        "\n",
        "first_out4 = Flatten() (MaxPooling1D(pool_size=pool_size) (encoded_left4))\n",
        "second_out4 = Flatten() (MaxPooling1D(pool_size=pool_size) (encoded_right4))\n",
        "\n",
        "merged4 = Concatenate(axis=1)([first_out4, second_out4])\n",
        "\n",
        "\n",
        "merged = Concatenate(axis=1) ([merged1, merged2, merged3, merged4])\n",
        "# merged = Concatenate(axis=1) ([merged1, merged2, merged3])\n",
        "output_layer1 = Dense(128, activation='relu')(merged)\n",
        "output_layer2 = Dense(32, activation='relu')(output_layer1)\n",
        "output_layer3 = Dense(1, activation='sigmoid')(output_layer2)\n",
        "\n",
        "model = Model(inputs=[left_input, right_input], outputs=output_layer3)\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['accuracy', get_f1, recall, precision])\n",
        "model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor=\"val_get_f1\", mode='max', patience=2, verbose=True)\n",
        "training_start_time = time()\n",
        "\n",
        "model.fit([X_train1, X_train2], Y_train, epochs=n_epoch, validation_data=([X_test1, X_test2], Y_test),callbacks = [early_stopping, ModelCheckpoint(filepath=model_path, save_best_only=True)])\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           (None, 41)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           (None, 41)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 41, 512)      5279744     input_11[0][0]                   \n",
            "                                                                 input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 41, 512)      0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 41, 512)      0           embedding_6[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 37, 128)      327808      dropout_1[0][0]                  \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 39, 128)      196736      dropout_1[0][0]                  \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 38, 128)      262272      dropout_1[0][0]                  \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 36, 128)      393344      dropout_1[0][0]                  \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 9, 128)       0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 9, 128)       0           conv1d_1[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 9, 128)       0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 9, 128)       0           conv1d_2[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 9, 128)       0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 9, 128)       0           conv1d_3[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1D)  (None, 9, 128)       0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, 9, 128)       0           conv1d_4[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 1152)         0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 1152)         0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 1152)         0           max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 1152)         0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1152)         0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 1152)         0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 1152)         0           max_pooling1d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_8 (Flatten)             (None, 1152)         0           max_pooling1d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2304)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 2304)         0           flatten_3[0][0]                  \n",
            "                                                                 flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 2304)         0           flatten_5[0][0]                  \n",
            "                                                                 flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 2304)         0           flatten_7[0][0]                  \n",
            "                                                                 flatten_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 9216)         0           concatenate_1[0][0]              \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 concatenate_3[0][0]              \n",
            "                                                                 concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          1179776     concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           4128        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            33          dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 7,643,841\n",
            "Trainable params: 7,643,841\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 47723 samples, validate on 11931 samples\n",
            "Epoch 1/10\n",
            "47723/47723 [==============================] - 65s 1ms/step - loss: 0.6060 - accuracy: 0.6593 - get_f1: 0.6367 - recall: 0.6348 - precision: 0.6731 - val_loss: 0.6177 - val_accuracy: 0.6492 - val_get_f1: 0.6018 - val_recall: 0.5572 - val_precision: 0.6930\n",
            "Epoch 2/10\n",
            "47723/47723 [==============================] - 56s 1ms/step - loss: 0.5112 - accuracy: 0.7287 - get_f1: 0.7247 - recall: 0.7391 - precision: 0.7275 - val_loss: 0.6617 - val_accuracy: 0.6340 - val_get_f1: 0.5448 - val_recall: 0.4677 - val_precision: 0.7111\n",
            "Epoch 3/10\n",
            "47723/47723 [==============================] - 56s 1ms/step - loss: 0.4068 - accuracy: 0.7956 - get_f1: 0.7978 - recall: 0.8304 - precision: 0.7786 - val_loss: 0.7572 - val_accuracy: 0.6471 - val_get_f1: 0.6314 - val_recall: 0.6222 - val_precision: 0.6645\n",
            "Epoch 4/10\n",
            "47723/47723 [==============================] - 56s 1ms/step - loss: 0.2993 - accuracy: 0.8596 - get_f1: 0.8597 - recall: 0.8840 - precision: 0.8448 - val_loss: 0.8074 - val_accuracy: 0.6201 - val_get_f1: 0.6097 - val_recall: 0.6092 - val_precision: 0.6312\n",
            "Epoch 5/10\n",
            "47723/47723 [==============================] - 56s 1ms/step - loss: 0.2253 - accuracy: 0.9022 - get_f1: 0.9008 - recall: 0.9124 - precision: 0.8958 - val_loss: 1.0279 - val_accuracy: 0.6260 - val_get_f1: 0.5859 - val_recall: 0.5500 - val_precision: 0.6564\n",
            "Epoch 00005: early stopping\n",
            "Training time finished.\n",
            "10 epochs in 0:05:11.499597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIGZ6MQGi-kP",
        "outputId": "868188f2-cc51-43a0-86d5-a470d9847675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# result = model.predict([X_test1, X_test2])\n",
        "# print(result)\n",
        "model.evaluate([X_test1, X_test2], Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11931/11931 [==============================] - 30s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8631682172224054,\n",
              " 0.6238370537757874,\n",
              " 0.5868266224861145,\n",
              " 0.5537066459655762,\n",
              " 0.6508835554122925]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}